{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e67a990",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (282684423.py, line 121)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[12], line 121\u001b[1;36m\u001b[0m\n\u001b[1;33m    else:\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch_geometric.data import Batch, Data\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def get_df(data_split=None, small=None):\n",
    "    \"\"\"\n",
    "    Read and returns a dataframe of the FactKG dataset.\n",
    "\n",
    "    Args:\n",
    "        data_split (str): Which datasplit to load, in `train`, `val` or `test`\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If `data_split` is an unsuported string.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame of the dataset.\n",
    "    \"\"\"\n",
    "    choices = [\"train\", \"val\", \"test\"]\n",
    "    data_split='test'\n",
    "\n",
    "\n",
    "    path ='E:\\\\factkg_test.pickle'\n",
    "    df = pd.DataFrame.from_dict(pd.read_pickle(path), orient=\"index\")\n",
    "    df.reset_index(inplace=True)  # Fix so sentences are a column, not index\n",
    "    df.rename(columns={\"index\": \"Sentence\"}, inplace=True)\n",
    "\n",
    "    if small:\n",
    "        df = df[:SMALL_SIZE]\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_precomputed_embeddings():#just get the embedding before\n",
    "    \"\"\"\n",
    "    Gets dict with precomputed embeddings, made with `make_subgraph_embeddings.py`.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If `data_split` is an unsuported string.\n",
    "\n",
    "    Returns:\n",
    "        dict: The dict of the subgraphs (as strings).\n",
    "    \"\"\"\n",
    "    path = Path(DATA_PATH) / EMBEDDINGS_FILENAME\n",
    "    embedding_dict = pickle.load(open(path, \"rb\"))\n",
    "    return embedding_dict\n",
    "\n",
    "\n",
    "class FactKGDataset(Dataset):\n",
    "    def __init__(self, df, evidence=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df (pd.DataFrame): Dataframe with claims (\"Sentence\") and labels (\"Label\").\n",
    "            evidence (list, optional): List of the subgraph evidences to use, will be converted to string. `None`\n",
    "                if no evidence should be used.\n",
    "        \"\"\"\n",
    "\n",
    "        self.inputs = df[\"Sentence\"]\n",
    "        self.labels = [int(label[0]) for label in df[\"Label\"]]\n",
    "        self.length = len(df)\n",
    "\n",
    "        if evidence is not None:\n",
    "            self.inputs = [self.inputs[i] + \" || \" + str(evidence[i]) for i in range(self.length)]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.labels[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "\n",
    "class CollateFunctor:\n",
    "    def __init__(self, tokenizer, max_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        inputs, labels = zip(*batch)\n",
    "        labels = torch.tensor(labels)\n",
    "        inputs = self.tokenizer(inputs, return_tensors=\"pt\", padding=True, truncation=True, max_length=self.max_length)\n",
    "        inputs[\"labels\"] = torch.as_tensor(labels)\n",
    "        return inputs\n",
    "\n",
    "\n",
    "def get_dataloader(data_split, subgraph_df, model=\"bert-base-uncased\",\n",
    "                   max_length=512, batch_size=64, shuffle=True, drop_last=True):\n",
    "    \"\"\"\n",
    "    Creates a dataloader for the desired data split and evidence (subgraph).\n",
    "\n",
    "    Args:\n",
    "        data_split (str): Which datasplit to load, in `train`, `val` or `test`\n",
    "        subgraph_type (str): The type of subgraph to use. Must be either `direct` (only the direct entity\n",
    "            neighbours), `direct_filled` (the direct entity neigbhours, but if it is empty, replace it with\n",
    "            all of the entity edges if the entities) or `one_hop` (all of the entity edges).\n",
    "        subgraph_to_use (str). In [\"discovered\", \"connected\", \"walkable\"]. \"discovered\" means that we use the string\n",
    "            representation of what directly found with `subgraph_type`. \"Connected\" means that walk the nodes and\n",
    "            relations found with `subgraph_type`, and use the connected graphs if found, and the walkable if not.\n",
    "            \"walkable\" means we use both the connected graphs and the walkable graphs.\n",
    "        model (str, optional): Name of model, in order to get tokenizer. Defaults to \"bert-base-uncased\".\n",
    "        max_length (int, optional): Max tokenizer length. Defaults to 512.\n",
    "        batch_size (int, optional): Batch size to dataloader. Defaults to 128.\n",
    "        shuffle (bool, optional): Shuffle dataset. Defaults to True.\n",
    "        drop_last (bool, optional): Drop last batch if it is less than `batch_size`. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        DataLoader: The dataloader.\n",
    "    \"\"\"\n",
    "    df = get_df()\n",
    "    #if subgraph_type is not None:\n",
    "      #  subgraphs = get_subgraphs(data_split, subgraph_type)\n",
    "      #  choices = [\"discovered\", \"connected\", \"walkable\"]\n",
    "      #  if subgraph_to_use not in choices:\n",
    "      #      raise ValueError(f\"Argument `subgraph_to_use` must be in {choices}. Was {subgraph_to_use}. \")\n",
    "      #  if subgraph_to_use == \"discovered\":\n",
    "       #     evidence = subgraphs[\"subgraph\"]\n",
    "    evidence = subgraphs_df[\"subgraph\"]\n",
    "    else:\n",
    "        evidence = None\n",
    "\n",
    "    dataset = FactKGDataset(df, evidence)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    collate_func = CollateFunctor(tokenizer, max_length)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last,\n",
    "                            collate_fn=collate_func)\n",
    "    return dataloader\n",
    "\n",
    "BERT_LAST_LAYER_DIM=[768]\n",
    "def get_embedding(text, embeddings_dict, tokenizer, model):\n",
    "\n",
    "    if embeddings_dict.get(text) is not None:\n",
    "        return torch.tensor(embeddings_dict[text])\n",
    "    return torch.zeros(BERT_LAST_LAYER_DIM)\n",
    "\n",
    "\n",
    "def convert_to_pyg_format(graph, online_embeddings, embedding_dict=None, tokenizer=None, model=None):\n",
    "    \"\"\"\n",
    "    Convert graph on DBpedia dict format to torch_embedding.data format, so it can be run in GNN.\n",
    "\n",
    "    Args:\n",
    "        graph (dict): Dict of graph, gotten by calling `kg.search()` on each element in the graph.\n",
    "        online_embeddings (bool): If True, will calculate embeddings for knowledge subgraph online, with a model\n",
    "                that might be tuned during the training.\n",
    "        embedding_dict (dict): Dict mapping words to embeddings, to be used as node and edge features.\n",
    "            This should be precomputed.\n",
    "        tokenizer (tokenizer): Tokenizer to `model` if `online_embeddings`.\n",
    "        model (pytroch model): Model to compute embeddings if `online_embeddings`.\n",
    "\n",
    "    Returns:\n",
    "        torch_geometric.data: Graph data.\n",
    "    \"\"\"\n",
    "\n",
    "    if graph == []:  # Dummy empty graph. Not actually empty because of vectorized computations.\n",
    "        graph = [[\"none\", \"none\", \"none\"]]\n",
    "    node_to_index = {}  # Node text to int mapping\n",
    "    edge_to_index = {}  # Same for edges\n",
    "    node_features = []  # List of embeddings\n",
    "    edge_features = []  # Same for edges\n",
    "    edge_indices = []\n",
    "\n",
    "    current_node_idx = 0\n",
    "    current_edge_idx = 0\n",
    "    for edge_list in graph:\n",
    "        node1, edge, node2 = edge_list  # Graph consists of list on the format [node1, edge, node2]\n",
    "\n",
    "        if node1 not in node_to_index:\n",
    "            node_to_index[node1] = current_node_idx\n",
    "            embedding = get_embedding(node1, embeddings_dict=embedding_dict,\n",
    "                                      tokenizer=tokenizer, model=model)\n",
    "            node_features.append(embedding)\n",
    "            current_node_idx += 1\n",
    "\n",
    "        if node2 not in node_to_index:\n",
    "            node_to_index[node2] = current_node_idx\n",
    "            embedding = get_embedding(node2, embeddings_dict=embedding_dict,\n",
    "                                      tokenizer=tokenizer, model=model)\n",
    "            node_features.append(embedding)\n",
    "            current_node_idx += 1\n",
    "\n",
    "        if edge not in edge_to_index:\n",
    "            edge_to_index[edge] = current_edge_idx\n",
    "            embedding = get_embedding(edge, embeddings_dict=embedding_dict,\n",
    "                                      tokenizer=tokenizer, model=model)\n",
    "            edge_features.append(embedding)\n",
    "            current_edge_idx += 1\n",
    "\n",
    "        edge_indices.append([node_to_index[node1], node_to_index[node2]])\n",
    "\n",
    "    edge_index = torch.tensor(edge_indices).t().contiguous()  # Transpose and make memory contigious\n",
    "    x = torch.stack(node_features)\n",
    "    edge_attr = torch.stack(edge_features)\n",
    "\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "    return data\n",
    "\n",
    "\n",
    "class FactKGDatasetGraph(Dataset):\n",
    "    def __init__(self, df, evidence, embedding_dict=None, tokenizer=None, model=None):\n",
    "        \"\"\"\n",
    "        Initialize the dataset. This dataset will return tokenized claims, graphs for the subgraph, and labels.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): FactKG dataframe\n",
    "            evidence (pd.DataFram): Dataframe with the subgraphs, found by `retrieve_subgraphs.py`.\n",
    "            online_embeddings (bool): If True, will calculate embeddings for knowledge subgraph online, with a model\n",
    "                that might be tuned during the training.\n",
    "            embedding_dict (dict): Dict mapping the knowledge graph words to embeddings if not `online_embeddings`.\n",
    "            tokenizer (tokenizer): Tokenizer to `model` if `online_embeddings`.\n",
    "            model (pytroch model): Model to compute embeddings if `online_embeddings`.\n",
    "            mix_graphs (bool, optional): If `True`, will use both the connected and the walkable graphs found in\n",
    "                DBpedia. If `False`, will use connected if it is not empty, else walkable. Defaults to False.\n",
    "        \"\"\"\n",
    "        self.inputs = df[\"Sentence\"]\n",
    "        self.labels = [int(label[0]) for label in df[\"Label\"]]\n",
    "        self.length = len(df)\n",
    "        self.subgraphs = evidence\n",
    "        self.online_embeddings = online_embeddings\n",
    "        self.embedding_dict = embedding_dict\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.mix_graphs = mix_graphs\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        claims = self.inputs[idx]\n",
    "        subgraph = self.subgraphs[idx]\n",
    "        graph = convert_to_pyg_format(\n",
    "            subgraph, online_embeddings=self.online_embeddings, embedding_dict=self.embedding_dict,\n",
    "            tokenizer=self.tokenizer, model=self.model)\n",
    "\n",
    "        label = self.labels[idx]\n",
    "        return claims, graph, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "\n",
    "class GraphCollateFunc:\n",
    "    def __init__(self, tokenizer, max_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        inputs, graph_batch, labels = zip(*batch)\n",
    "        tokens = self.tokenizer(inputs, return_tensors=\"pt\", padding=True, truncation=True,\n",
    "                                max_length=self.max_length)\n",
    "        graph_batch = Batch.from_data_list(graph_batch)\n",
    "        labels = torch.tensor(labels).float()\n",
    "\n",
    "        return tokens, graph_batch, labels\n",
    "subgraph_path=\"E:\\\\test_combined_subgraphs.pkl\"\n",
    "with open(subgraph_path, 'rb') as file:\n",
    "    subgraphs = pickle.load(file)\n",
    "EMBEDDINGS_FILENAME = \"embeddings_testset.pkl\"\n",
    "DATA_PATH = \"/content/drive/My Drive/NLP_Lab/\"\n",
    "embedding_path= Path(DATA_PATH) / EMBEDDINGS_FILENAME\n",
    "with open(embedding_path, 'rb') as file:\n",
    "    embedding_dict = pickle.load(file)\n",
    "\n",
    "def get_graph_dataloader(\n",
    "        data_split, subgraphs, embedding_dict,model=None, bert_model_name=\"bert-base-uncased\",\n",
    "        max_length=512, batch_size=64, shuffle=True, drop_last=True, mix_graphs=False):\n",
    "    \"\"\"\n",
    "    Creates a dataloader for dataset with subgraph representation and tokenized text.\n",
    "\n",
    "    Args:\n",
    "        data_split (str): Which datasplit to load, in `train`, `val` or `test`\n",
    "        subgraph_type (str): The type of subgraph to use. Must be either `direct` (only the direct entity\n",
    "            neighbours), `direct_filled` (the direct entity neigbhours, but if it is empty, replace it with\n",
    "            all of the entity edges if the entities), `one_hop` (all of the entity edges) or `relevant` (direct plus\n",
    "            edges that appears in claim).\n",
    "        online_embeddings (bool): If True, will calculate embeddings for knowledge subgraph online, with a model\n",
    "                that might be tuned during the training.\n",
    "        bert_model_name (str, optional): Name of model, in order to get tokenizer. Defaults to \"bert-base-uncased\".\n",
    "        max_length (int, optional): Max tokenizer length. Defaults to 512.\n",
    "        batch_size (int, optional): Batch size to dataloader. Defaults to 128.\n",
    "        shuffle (bool, optional): Shuffle dataset. Defaults to True.\n",
    "        drop_last (bool, optional): Drop last batch if it is less than `batch_size`. Defaults to True.\n",
    "        mix_graphs (bool, optional): If `True`, will use both the connected and the walkable graphs found in\n",
    "                DBpedia. If `False`, will use connected if it is not empty, else walkable. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        DataLoader: The dataloader.\n",
    "    \"\"\"\n",
    "    df = get_df(data_split)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "    graph_collate_func = GraphCollateFunc(tokenizer, max_length=max_length)\n",
    "    embedding_dict = embedding_dict\n",
    "    dataset = FactKGDatasetGraph(\n",
    "            df, subgraphs, embedding_dict=embedding_dict, mix_graphs=mix_graphs)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last,\n",
    "                            collate_fn=graph_collate_func)\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e49bf546",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import cosine_similarity\n",
    "from torch_geometric.nn import GATv2Conv, global_mean_pool\n",
    "from transformers import AutoModelForSequenceClassification, AutoModel\n",
    "\n",
    "\n",
    "\n",
    "def get_bert_model(model_name=\"bert\", include_classifier=True, num_labels=2, freeze_base_model=False,\n",
    "                   freeze_up_to_pooler=True, dropout_rate=0, use_roberta=False):\n",
    "    \"\"\"\n",
    "    Load a pretrained BERT model with desired configurations.\n",
    "\n",
    "    Args:\n",
    "        model_name (str, optional): Name of the model, will be saved as a class variable. Defaults to \"bert\".\n",
    "        include_classifier (bool, optional): Include a classification layer. Defaults to True.\n",
    "        num_labels (int, optional): Number of outputs if classification layer is included. Defaults to 2.\n",
    "        freeze_base_model (bool, optional): Will freeze all layers up to the classification layer. Defaults to False.\n",
    "        freeze_up_to_pooler (bool, optional): Will freeze all layers until the last layer in BERT, the pooler,\n",
    "            with approximately 500k parameters. Defaults to True.\n",
    "        dropout_rate (int, optional): Dropout rate for the classification layer. Defaults to 0.\n",
    "        use_roberta (bool): If `True`, will use RoBERTa instead of BERT.\n",
    "\n",
    "    Returns:\n",
    "        transformer model: The loaded model.\n",
    "    \"\"\"\n",
    "    if freeze_base_model and freeze_up_to_pooler:\n",
    "        logger.warn(\"Both `freeze_base_model` and `freeze_up_to_pooler` is True. Freezing base model.\")\n",
    "\n",
    "    if use_roberta:\n",
    "        model_name = \"roberta-base\"\n",
    "    else:\n",
    "        model_name = \"bert-base-uncased\"\n",
    "\n",
    "    if include_classifier:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name, cache_dir=\"./cache\", trust_remote_code=True, num_labels=num_labels,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "    else:\n",
    "        model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    model.name = model_name\n",
    "    if freeze_base_model:\n",
    "        for params in model.base_model.parameters():\n",
    "            params.requires_grad = False\n",
    "    elif freeze_up_to_pooler:\n",
    "        for name, params in model.base_model.named_parameters():\n",
    "            if not name.startswith(\"pooler\"):\n",
    "                params.requires_grad = False\n",
    "\n",
    "    if dropout_rate != 0:\n",
    "        classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(model.classifier.in_features, model.classifier.out_features)\n",
    "        )\n",
    "        model.classifier = classifier\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "class QAGNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of Quastion Answer Graph Neural Network model\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name, n_gnn_layers=2, gnn_hidden_dim=256, gnn_out_features=256, lm_layer_features=None,\n",
    "                 gnn_batch_norm=True, freeze_base_model=False, freeze_up_to_pooler=True, gnn_dropout=0.3,\n",
    "                 classifier_dropout=0.2, lm_layer_dropout=0.4, use_roberta=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model_name (str): Name of the model, will be saved as a class variable.\n",
    "            n_gnn_layers (int, optional): Number of layers in the GNN. Defaults to 2.\n",
    "            gnn_hidden_dim (int, optional): Number of nodes in GNN layers. Defaults to 256.\n",
    "            gnn_out_features (int, optional): Number of output nodes from the GNN. Defaults to 256.\n",
    "            lm_layer_features (int): If not `None`, will add a linear layer after the claim embedding that will be\n",
    "                used in the classification layer, concatenated with the GNN output. The layer will have\n",
    "                `lm_layer_features` nodes, and a dropout of `lm_layer_dropout`. If `None`, will use the lm (bert)\n",
    "                embedding concatenated with the GNN output for the classification layer.\n",
    "            gnn_batch_norm (bool, optional): Whether or not to apply batch norm between the GNN layers.\n",
    "                Defaults to True.\n",
    "            freeze_base_model (bool, optional): Freeze the base model of the Bert language model. Defaults to False.\n",
    "            freeze_up_to_pooler (bool, optional): Freeze up to the last part of the Bert model. Defaults to True.\n",
    "            gnn_dropout (float, optional): Dropout rate for the GNN layers. Defaults to 0.3.\n",
    "            classifier_dropout (float, optional): Dropout rate for the last layer.\n",
    "            lm_layer_dropout (float, optional): Dropout rate for the optional `lm_layer`.\n",
    "            use_roberta (bool): If True, will use RoBERTa for the language model (the one that trains, not the\n",
    "                one for the embeddings.)\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If `n_gnn_layers` is less than 2.\n",
    "        \"\"\"\n",
    "        if n_gnn_layers < 2:\n",
    "            raise ValueError(f\"Argument `n_gnn_layers` must be atleast 2. Was {n_gnn_layers}. \")\n",
    "        super(QAGNN, self).__init__()\n",
    "\n",
    "        self.name = model_name\n",
    "        self.bert = get_bert_model(\"bert_\" + model_name, include_classifier=False, freeze_base_model=freeze_base_model,\n",
    "                                   freeze_up_to_pooler=freeze_up_to_pooler, use_roberta=use_roberta)\n",
    "\n",
    "        self.n_gnn_layers = n_gnn_layers\n",
    "        self.gnn_layers = nn.ModuleList()\n",
    "        first_gnn_layer = GATConv(self.bert.config.hidden_size, gnn_hidden_dim, dropout=gnn_dropout)\n",
    "        self.gnn_layers.append(first_gnn_layer)\n",
    "        for i in range(n_gnn_layers - 2):\n",
    "            gnn_layer = GATv2Conv(gnn_hidden_dim, gnn_hidden_dim, dropout=gnn_dropout)\n",
    "            self.gnn_layers.append(gnn_layer)\n",
    "        last_gnn_layer = GATv2Conv(gnn_hidden_dim, gnn_out_features, dropout=gnn_dropout)\n",
    "        self.gnn_layers.append(last_gnn_layer)\n",
    "\n",
    "        claim_dim = self.bert.config.hidden_size\n",
    "        self.with_lm_layer = False\n",
    "        if lm_layer_features is not None:\n",
    "            self.lm_dropout = nn.Dropout(lm_layer_dropout)\n",
    "            self.lm_layer = nn.Linear(self.bert.config.hidden_size, lm_layer_features)\n",
    "            claim_dim = lm_layer_features\n",
    "            self.with_lm_layer = True\n",
    "\n",
    "        self.gnn_batch_norm = gnn_batch_norm\n",
    "        if gnn_batch_norm:\n",
    "            self.gnn_batch_norm_layers = nn.ModuleList()\n",
    "            for i in range(n_gnn_layers - 1):\n",
    "                batch_norm_layer = nn.BatchNorm1d(gnn_hidden_dim)\n",
    "                self.gnn_batch_norm_layers.append(batch_norm_layer)\n",
    "\n",
    "        self.classsifier_dropout_layer = nn.Dropout(classifier_dropout)\n",
    "        self.classifier = nn.Linear(gnn_out_features + claim_dim, 1)\n",
    "\n",
    "    def forward(self, claim_tokens, data_graphs):\n",
    "        claim_outputs = self.bert(**claim_tokens)\n",
    "        claim_embeddings = claim_outputs.last_hidden_state[:, 0]  # Using the [CLS] token's embedding\n",
    "\n",
    "        batch = data_graphs\n",
    "        claim_embeddings_expanded = claim_embeddings[batch.batch]  # Expand to match batch size\n",
    "        relevance_scores = F.cosine_similarity(claim_embeddings_expanded, batch.x, dim=-1).unsqueeze(-1)\n",
    "        weighted_node_features = batch.x * relevance_scores\n",
    "\n",
    "        x = weighted_node_features\n",
    "        for i in range(self.n_gnn_layers):\n",
    "            x = self.gnn_layers[i](x, batch.edge_index)\n",
    "            if self.gnn_batch_norm and i < (self.n_gnn_layers - 1):\n",
    "                x = self.gnn_batch_norm_layers[i](x)\n",
    "            x = F.relu(x)\n",
    "\n",
    "        # Pooling the node features\n",
    "        pooled_gnn_output = global_mean_pool(x, batch.batch)  # Pool over all nodes in each graph\n",
    "\n",
    "        if self.with_lm_layer:\n",
    "            claim_embeddings = self.lm_dropout(claim_embeddings)\n",
    "            claim_embeddings = self.lm_layer(claim_embeddings)\n",
    "\n",
    "        combined_features = torch.cat((pooled_gnn_output, claim_embeddings), dim=1)\n",
    "\n",
    "        combined_features = self.classsifier_dropout_layer(combined_features)\n",
    "        out = self.classifier(combined_features)  # [batch_size, 1]\n",
    "\n",
    "        return out.squeeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c457cc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "N_EARLY_STOP_DEFAULT=3\n",
    "\n",
    "\n",
    "\n",
    "def run_epoch_simple(train, dataloader, optimizer, model, device):\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    for inputs in dataloader:\n",
    "        batch = inputs.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * batch[\"input_ids\"].size(0)\n",
    "        probabilities = torch.softmax(outputs.logits, dim=1)\n",
    "        preds = torch.argmax(probabilities, dim=1)\n",
    "        total_correct += (preds == batch[\"labels\"]).sum().item()\n",
    "    return total_loss, total_correct\n",
    "\n",
    "\n",
    "def run_epoch_qa_gnn(train, dataloader, optimizer, model, criterion, device):\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    for inputs, data_graph, labels in dataloader:\n",
    "        batch = inputs.to(device)\n",
    "        data_graph = data_graph.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch, data_graph)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * batch[\"input_ids\"].size(0)\n",
    "        probabilities = torch.sigmoid(outputs)\n",
    "        preds = (probabilities > 0.5).int()\n",
    "        total_correct += (preds == labels).sum().item()\n",
    "    return total_loss, total_correct\n",
    "\n",
    "\n",
    "def train(model, criterion, optimizer, qa_gnn, train_loader, val_loader=None, n_epochs=10, scheduler=None,\n",
    "          n_early_stop=None, save_models=True, device=None, non_blocking=False, verbose=1):\n",
    "    \"\"\"\n",
    "    Trains a model and calculate training and valudation stats, given the model, loader, optimizer\n",
    "    and some hyperparameters.\n",
    "\n",
    "    Args:\n",
    "        model (model): The model to train. Freeze layers ahead of calling this function.\n",
    "        criterion (callable): Pytorch loss function.\n",
    "        optimizer (optim): Pytorch Optimizer.\n",
    "        qa_gnn (bool): Wether the model is a QA-GNN model with graphs (True), or a language model (False).\n",
    "        train_loader (dataloader): Data loader for training set\n",
    "        val_loader (dataloader, optional): Optinal validation data loader.\n",
    "            If not None, will calculate validation loss and accuracy after each epoch.\n",
    "        n_epochs (int, optional): Amount of epochs to run. Defaults to 10.\n",
    "        scheduler (scheduler, optional): Optional learning rate scheduler.\n",
    "        n_early_stop (int): The number of consecutive iterations without validation loss improvement that\n",
    "            stops the training (early stopping). Will only work if `val_loader` is None.\n",
    "            Set to `False` for deactivating it, and `None` for default value from `constants.py`.\n",
    "        save_models (bool): If True and `val_loader` is not None, will save the best models state dicts.\n",
    "        non_blocking (bool): If True, allows for asyncronous transfer between RAM and VRAM.\n",
    "            This only works together with `pin_memory=True` to dataloader and GPU training.\n",
    "        verbose (int): If 0, will not log anything. If not 0, will log last epoch with INFO and the others with DEBUG.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of the training history. Will contain lists of training loss and accuracy over\n",
    "            epochs, and for validation loss and accuracy if `val_loader` is not None.\n",
    "        dict: A dictionary with trained model, and optional best-model state-dicts if `val_loader` is not None.\n",
    "            On the form: {\"final_model\": model, \"best_model_accuracy_state_dict\": a, \"best_model_loss_state_dict\": b}\n",
    "    \"\"\"\n",
    "    if n_early_stop is None:\n",
    "        n_early_stop = N_EARLY_STOP_DEFAULT\n",
    "    elif not n_early_stop:\n",
    "        n_early_stop = n_epochs\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device, non_blocking=non_blocking)\n",
    "    train_class_loss_list = []  # Initialize training history variables\n",
    "    train_class_accuracy_list = []\n",
    "    val_class_loss_list = []  # These will remain empty if `val_loader` is None\n",
    "    val_class_accuracy_list = []\n",
    "    best_epoch_number = -1\n",
    "    best_val_loss = np.inf\n",
    "    best_val_accuracy = -1\n",
    "    best_model = None  # This will only be saved if `val_loader` is not None\n",
    "    n_stagnation = 0\n",
    "    if verbose != 0:\n",
    "        logger.info(f\"Starting training with device {device}.\")\n",
    "\n",
    "    for epoch in range(n_epochs):  # Train\n",
    "        if qa_gnn:\n",
    "            train_loss, train_correct = run_epoch_qa_gnn(\n",
    "                train=True, dataloader=train_loader, optimizer=optimizer, model=model,\n",
    "                criterion=criterion, device=device)\n",
    "        else:\n",
    "            train_loss, train_correct = run_epoch_simple(\n",
    "                train=True, dataloader=train_loader, optimizer=optimizer, model=model, device=device)\n",
    "        average_train_loss = train_loss / len(train_loader.dataset)\n",
    "        train_accuracy = 100 * train_correct / len(train_loader.dataset)\n",
    "        train_class_loss_list.append(average_train_loss)\n",
    "        train_class_accuracy_list.append(train_accuracy)\n",
    "\n",
    "        if val_loader is not None:  # Eval\n",
    "            with torch.no_grad():\n",
    "                if qa_gnn:\n",
    "                    val_loss, val_correct = run_epoch_qa_gnn(\n",
    "                        train=False, dataloader=val_loader, optimizer=optimizer, model=model,\n",
    "                        criterion=criterion, device=device)\n",
    "                else:\n",
    "                    val_loss, val_correct = run_epoch_simple(\n",
    "                        train=False, dataloader=val_loader, optimizer=optimizer, model=model, device=device)\n",
    "\n",
    "                average_val_loss = val_loss / len(val_loader.dataset)\n",
    "                val_accuracy = 100 * val_correct / len(val_loader.dataset)\n",
    "                val_class_loss_list.append(average_val_loss)\n",
    "                val_class_accuracy_list.append(val_accuracy)\n",
    "\n",
    "                if average_val_loss >= best_val_loss:  # Check for stagnation _before_ updating best_val_loss\n",
    "                    n_stagnation += 1\n",
    "                else:  # Better than best loss\n",
    "                    n_stagnation = 0\n",
    "                if n_stagnation == n_early_stop:  # Early stopping, abort training\n",
    "                    if verbose == 0:  # No output\n",
    "                        break\n",
    "                    print(f\"Epoch [{epoch + 1} / {n_epochs}]\\n\")\n",
    "                    print(f\"Train loss: {average_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}%\")\n",
    "                    print(f\"Validation loss: {average_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}%\\n\")\n",
    "                    print(f\"Early stopping after {n_stagnation} rounds of no validation loss improvement.\\n\")\n",
    "                    break\n",
    "\n",
    "                if average_val_loss < best_val_loss:\n",
    "                    best_val_loss = average_val_loss\n",
    "                    best_epoch_number = epoch + 1\n",
    "                    best_val_accuracy = val_accuracy\n",
    "                    best_model = model.state_dict()\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        if verbose == 0:  # Do not log\n",
    "            continue\n",
    "\n",
    "        message = f\"Epoch [{epoch + 1} / {n_epochs}]\\n\"\n",
    "        message += f\"Train loss: {average_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}%\\n\"\n",
    "        if val_loader is not None:\n",
    "            message += f\"Validation loss: {average_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}%\"\n",
    "        message += \"\\n\"\n",
    "\n",
    "        if epoch + 1 == n_epochs:  # Last epoch\n",
    "            print(message)\n",
    "        else:\n",
    "            print(message)\n",
    "\n",
    "    history = {\"train_class_loss\": train_class_loss_list, \"train_class_accuracy\": train_class_accuracy_list,\n",
    "               \"val_class_loss\": val_class_loss_list, \"val_class_accuracy\": val_class_accuracy_list,\n",
    "               \"best_epoch\": best_epoch_number, \"best_val_accuracy\": best_val_accuracy,\n",
    "               \"best_val_loss\": best_val_loss, \"model_name\": model.name}\n",
    "\n",
    "    models_dict = {\"final_model\": model}\n",
    "    if val_loader is not None:\n",
    "        models_dict[\"best_model_state_dict\"] = best_model\n",
    "        if save_models:\n",
    "            save_model(best_model, model.name)\n",
    "\n",
    "    return history, models_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07555ea6",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (580355891.py, line 30)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[15], line 30\u001b[1;36m\u001b[0m\n\u001b[1;33m    lr_scheduler = transformers.get_linear_schedule_with_warmup(\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from datasets import get_dataloader, get_graph_dataloader\n",
    "from evaluate import evaluate_on_test_set\n",
    "#from glocal_settings import SMALL, SMALL_SIZE\n",
    "#from models import QAGNN, get_bert_model\n",
    "#from train import train\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "seed_everything(57)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = QAGNN(\n",
    "            args.model_name, n_gnn_layers=args.n_gnn_layers, gnn_hidden_dim=args.gnn_hidden_dim,\n",
    "            gnn_out_features=args.gnn_out_features, lm_layer_features=args.lm_layer_features,\n",
    "            gnn_batch_norm=args.gnn_batch_norm, freeze_base_model=args.freeze_base_model,\n",
    "            freeze_up_to_pooler=args.freeze_up_to_pooler, gnn_dropout=args.gnn_dropout,\n",
    "            classifier_dropout=args.classifier_dropout, lm_layer_dropout=args.lm_layer_dropout,\n",
    "            use_roberta=args.use_roberta)\n",
    "        \n",
    "train_loader = get_graph_dataloader(\n",
    "            \"train\", subgraph_type=args.subgraph_type, online_embeddings=args.online_embeddings,\n",
    "            model=embedding_model, batch_size=args.batch_size, mix_graphs=args.mix_graphs)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=args.learning_rate)\n",
    "lr_scheduler = transformers.get_linear_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=50, num_training_steps=len(train_loader) * args.n_epochs\n",
    "        )\n",
    "\n",
    "history, models_dict = train(\n",
    "            model=model, criterion=criterion, optimizer=optimizer, qa_gnn=args.qa_gnn, train_loader=train_loader,\n",
    "            val_loader=val_loader, n_epochs=args.n_epochs, scheduler=lr_scheduler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8db758",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pyg_env)",
   "language": "python",
   "name": "pyg_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
