{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "98fd5f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the pickle file\n",
    "input_path = \"E://testset_filtered.pickle\"\n",
    "with open(input_path, 'rb') as file:\n",
    "    test = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d75bacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from random import choice\n",
    "from itertools import chain\n",
    "\n",
    "# Load the Knowledge Graph from dbpedia.pickle\n",
    "with open('E://testset_filtered_data.pickle', 'rb') as file:\n",
    "    testfiltered = pickle.load(file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3060320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity_set': ['Mobyland'], 'evidence_onehop_full': None, 'label': [True], 'reasoning_types': ['existence'], 'evidence_filtered': {'Mobyland': ['successor']}, 'direct': None, 'filtered': {'Mobyland': {'successor': ['\"Aero 2\"']}}}\n",
      "{'entity_set': ['Dawn_Butler'], 'evidence_onehop_full': None, 'label': [True], 'reasoning_types': ['existence'], 'evidence_filtered': {'Dawn_Butler': ['successor', 'office', 'predecessor', 'website', 'primeminister']}, 'direct': None, 'filtered': {'Dawn_Butler': {'successor': ['\"Office Vacant\"', 'Boundary_Commissions_(United_Kingdom)'], 'office': ['Member_of_parliament', '\"forBrent Central\"', '\"for Brent Central\"'], 'predecessor': ['Paul_Boateng', '\"Office Created\"', 'Sarah_Teather'], 'website': ['1489', ''], 'primeminister': ['Gordon_Brown']}}}\n",
      "{'entity_set': ['Joseph_Brunton'], 'evidence_onehop_full': None, 'label': [True], 'reasoning_types': ['existence'], 'evidence_filtered': {'Joseph_Brunton': ['predecessor', 'successor', 'deathYear', 'birthYear', 'placeOfDeath', 'deathDate']}, 'direct': None, 'filtered': {'Joseph_Brunton': {'predecessor': ['Arthur_A._Schuck'], 'successor': ['Alden_G._Barber'], 'deathYear': ['\"1988\"'], 'birthYear': ['\"1902\"'], 'placeOfDeath': ['Gibsonia,_Pennsylvania'], 'deathDate': ['\"1988-07-08\"']}}}\n",
      "{'entity_set': ['Stubb_Cabinet'], 'evidence_onehop_full': None, 'label': [True], 'reasoning_types': ['existence'], 'evidence_filtered': {'Stubb_Cabinet': ['successor', 'jurisdiction', 'title']}, 'direct': None, 'filtered': {'Stubb_Cabinet': {'successor': ['Sipilä_Cabinet'], 'jurisdiction': ['Finland'], 'title': ['\"Minister for European Affairs and Foreign Trade\"', 'Cabinet_(government)', '\"Minister of Agriculture and Forestry\"']}}}\n",
      "{'entity_set': ['John_Sherman_Cooper'], 'evidence_onehop_full': None, 'label': [True], 'reasoning_types': ['existence'], 'evidence_filtered': {'John_Sherman_Cooper': ['party', 'profession', 'serviceStartYear', 'rank', '~ successor']}, 'direct': None, 'filtered': {'John_Sherman_Cooper': {'party': ['Republican_Party_(United_States)'], 'profession': ['\"Lawyer\"'], 'serviceStartYear': ['\"1942\"'], 'rank': ['Captain_(United_States_O-3)']}}}\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "j=0\n",
    "count=0\n",
    "for claim, information in testfiltered.items():\n",
    "    if j<=4:\n",
    "        print(information)\n",
    "        j+=1\n",
    "    else:\n",
    "        break\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4ade55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('E:\\dbpedia_2015_undirected_light.pickle', 'rb') as file:\n",
    "    dbpedia = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "43339f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surgical_positions could not be found\n",
      "Vehicle_product_list could not be found\n",
      "Law_of_graduality could not be found\n",
      "Natural_units could not be found\n",
      "Findstr could not be found\n",
      "Govan_High_School could not be found\n",
      "Kanakesa_Thevar could not be found\n",
      "Sunlight_Electric could not be found\n",
      "Joan_Heggen could not be found\n",
      "Darby_Dunn could not be found\n",
      "Mourya_Sawant could not be found\n",
      "Ark_Performance could not be found\n",
      "Aureo_Comamala could not be found\n",
      "Krisztián_Soós could not be found\n",
      "Dedy_Ariyadi_Jumaidi could not be found\n",
      "Alla_Malchyk could not be found\n",
      "Yuri_Romanyuk could not be found\n",
      "Ramiro_Martínez could not be found\n",
      "Kofi_Maxwell_Nyarko could not be found\n",
      "RJ_Noll could not be found\n",
      "Fife_Opera could not be found\n",
      "WIBISCO could not be found\n",
      "Hincks_and_Burnell could not be found\n",
      "Paas could not be found\n",
      "241_Pizza could not be found\n",
      "Era_of_Silence could not be found\n",
      "Nshenyi could not be found\n",
      "Black_hole_electron could not be found\n",
      "Amurru_kingdom could not be found\n",
      "Brungaria could not be found\n",
      "Garmul could not be found\n",
      "Blue_hour could not be found\n",
      "Sonni_Dynasty could not be found\n",
      "Gummosis could not be found\n",
      "10K_Plan could not be found\n",
      "Kachin_Levies could not be found\n",
      "Gengda_Township could not be found\n",
      "Optical_circulator could not be found\n",
      "Capricorn_Assemblage could not be found\n",
      "INAS_342 could not be found\n",
      "Figure_skating_jumps could not be found\n",
      "Raleigh_School could not be found\n",
      "Govan_High_School could not be found\n",
      "December_12 could not be found\n",
      "September_12 could not be found\n",
      "Piazza_Colonna could not be found\n",
      "Jennifer_Plass could not be found\n",
      "Jose_Antonio_Pineda could not be found\n",
      "Rosemary_Hollis could not be found\n",
      "Ryder_Windham could not be found\n",
      "Musekiwa_Chingodza could not be found\n",
      "Nikolaos_Morakis could not be found\n",
      "Isaac_(Khazar) could not be found\n",
      "Husky_the_Muskie could not be found\n",
      "Brett_Sports_F.C. could not be found\n",
      "Svafrlami could not be found\n",
      "Joel_Djondang could not be found\n",
      "Kani_Çevik could not be found\n",
      "Mal_Friedman could not be found\n",
      "Adonis_Shaqiri could not be found\n",
      "Arne_Weingart could not be found\n",
      "Ángel_Vasallo could not be found\n",
      "Walter_Andrade could not be found\n",
      "Christian_Bocher could not be found\n",
      "Rothesay_Academy could not be found\n",
      "Gábor_Neudl could not be found\n",
      "Sokimex could not be found\n",
      "Naheere could not be found\n",
      "Waskita_Karya could not be found\n",
      "Adzyubzha could not be found\n",
      "Sangama_dynasty could not be found\n",
      "Vigil could not be found\n",
      "Ordovician could not be found\n",
      "Orosirian could not be found\n",
      "Montehermosan could not be found\n",
      "Kilwa_Sultanate could not be found\n",
      "Siderian could not be found\n",
      "Equitable_conversion could not be found\n",
      "BALCO_scandal could not be found\n",
      "SIMCE could not be found\n",
      "Hollandsche_Manege could not be found\n",
      "August_9 could not be found\n",
      "PS_11_(Manhattan) could not be found\n",
      "September_6 could not be found\n",
      "Pershing_High_School could not be found\n",
      "Deborah_Akers could not be found\n",
      "Allen_S._Baker could not be found\n",
      "Zhang_Yaotiao could not be found\n",
      "Battle_buddy could not be found\n",
      "Hans_Brosamer could not be found\n",
      "Erwin_J._Haeberle could not be found\n",
      "Wally_Tatomir could not be found\n",
      "Olivier_Dokunengo could not be found\n",
      "Joel_Faga could not be found\n",
      "Sybil_Temchen could not be found\n",
      "Helen_Kay_Larson could not be found\n",
      "Stephane_Tchoumi could not be found\n",
      "Aleksandr_Tereshko could not be found\n",
      "0.514987280168123\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import random\n",
    "from itertools import chain\n",
    "\n",
    "output_path = 'E:\\\\test_direct.pickle'\n",
    "\n",
    "# Process all entities and save two-hop neighbors\n",
    "i=0\n",
    "test_adddirect={}\n",
    "for claim, information in test.items():\n",
    "    information['direct']={}\n",
    "    information['filtered']={}\n",
    "    direct_connection_found = False\n",
    "    for entity in information['entity_set']:\n",
    "        if entity in dbpedia:\n",
    "            information['direct'][entity]={}\n",
    "            for relation, neighbours in dbpedia[entity].items():\n",
    "                for neighbour in neighbours:\n",
    "                    if neighbour in information['entity_set']:\n",
    "                        information['direct'][entity][relation]=neighbours\n",
    "                        direct_connection_found = True\n",
    "        else:\n",
    "            print(f\"{entity} could not be found\")\n",
    "   # print(information)\n",
    "    if 'evidence_filtered' in information:\n",
    "        for entity_,relations in information['evidence_filtered'].items():\n",
    "            information['filtered'][entity_]={}\n",
    "            relations=information['evidence_filtered'][entity_]\n",
    "            for relation in relations: \n",
    "                if  relation in information['evidence_onehop_full'][entity_]:\n",
    "                    information['filtered'][entity_][relation]=random.sample(\n",
    "                    information['evidence_onehop_full'][entity_][relation],  # Source list\n",
    "                min(len(information['evidence_onehop_full'][entity_][relation]), 3)  # Max 3 items\n",
    "            )\n",
    "    else:\n",
    "        information['evidence_filtered']={}\n",
    "    information['evidence_onehop_full']=None\n",
    "    if not direct_connection_found:\n",
    "        information['direct'] = None\n",
    "    else:\n",
    "        i+=1\n",
    "        \n",
    "    test_adddirect[claim]=information\n",
    "print(i/len(test.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6adb85b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path_3= 'E://testset_filtered_data.pickle'\n",
    "with open(output_path_3, 'wb') as output_file:\n",
    "        pickle.dump(test_adddirect, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "561bc881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I have heard that Mobyland had a successor.', {'entity_set': ['Mobyland'], 'evidence_onehop_full': None, 'label': [True], 'reasoning_types': ['existence'], 'evidence_filtered': {'Mobyland': ['successor']}, 'direct': None, 'filtered': {'Mobyland': {'successor': ['\"Aero 2\"']}}})\n"
     ]
    }
   ],
   "source": [
    "for claim, information in enumerate(test_adddirect.items()):\n",
    "    print(information)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720fe093",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5f7f3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('E:\\\\test_direct.pickle', 'rb') as file:\n",
    "    test_add = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b6b856",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "def converting_subgraph(dic,method):\n",
    "    subgraphs=[]\n",
    "    for index, (claim, information) in enumerate(dic.items()):\n",
    "        subgraph={}\n",
    "        if method=='direct':\n",
    "            for entity,relations in information['direct'].items():\n",
    "                print('evidence_filtered')\n",
    "                subgraph[entity]=[]\n",
    "                for relation,neighbours in information['direct'][entity].items():\n",
    "                    subgraph[entity].append(relation)\n",
    "        if method=='combined':\n",
    "            if information['direct'] != None:\n",
    "                for entity,relations in information['direct'].items():\n",
    "                    subgraph[entity]=[]\n",
    "                    for relation,neighbours in information['direct'][entity].items():\n",
    "                        subgraph[entity].append(relation)\n",
    "            if 'filtered' in information:\n",
    "                    for entity_,relations_ in information['filtered'].items():\n",
    "                        if entity_ not in subgraph:\n",
    "                            subgraph[entity_]=[]\n",
    "                            for relation_ in information['evidence_filtered'][entity_]:\n",
    "                                subgraph[entity_].append(relation_)\n",
    "                        \n",
    "        else:\n",
    "            break\n",
    "        subgraphs.append(subgraph)\n",
    "        #print(f\"{index},{subgraph}\")\n",
    "    return subgraphs\n",
    "\n",
    "\n",
    "subgraphs=converting_subgraph(testfiltered,'combined')\n",
    "save_path = \"E:\\\\test_combined_sub.pkl\"\n",
    "subgraph_df = pd.DataFrame()\n",
    "subgraph_df['subgraph'] = subgraphs\n",
    "subgraph_df.to_pickle(save_path)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bdb9f540",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'to_pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 35\u001b[0m\n\u001b[0;32m     33\u001b[0m subgraphs\u001b[38;5;241m=\u001b[39mconverting_subgraph(testfiltered,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcombined\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     34\u001b[0m save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mE:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mtest_combined_subgraphs.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 35\u001b[0m \u001b[43msubgraphs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_pickle\u001b[49m(save_path)         \n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'to_pickle'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "def converting_subgraph(dic,method):\n",
    "    subgraphs=[]\n",
    "    for index, (claim, information) in enumerate(dic.items()):\n",
    "        subgraph={}\n",
    "        if method=='direct':\n",
    "            for entity,relations in information['direct'].items():\n",
    "                print('evidence_filtered')\n",
    "                subgraph[entity]=[]\n",
    "                for relation,neighbours in information['direct'][entity].items():\n",
    "                    subgraph[entity].append(relation)\n",
    "        if method=='combined':\n",
    "            if information['direct'] != None:\n",
    "                for entity,relations in information['direct'].items():\n",
    "                    subgraph[entity]=[]\n",
    "                    for relation,neighbours in information['direct'][entity].items():\n",
    "                        subgraph[entity].append(relation)\n",
    "            if 'evidence_filtered' in information:\n",
    "                    for entity_,relations_ in information['evidence_filtered'].items():\n",
    "                        if entity_ not in subgraph:\n",
    "                            subgraph[entity_]=[]\n",
    "                            for relation_ in information['evidence_filtered'][entity_]:\n",
    "                                subgraph[entity_].append(relation_)\n",
    "                        \n",
    "        else:\n",
    "            break\n",
    "        subgraphs.append(subgraph)\n",
    "        #print(f\"{index},{subgraph}\")\n",
    "    return subgraphs\n",
    "\n",
    "\n",
    "subgraphs=converting_subgraph(testfiltered,'combined')\n",
    "save_path = \"E:\\\\test_combined_subgraphs.pkl\"\n",
    "subgraph_df = pd.DataFrame()\n",
    "subgraph_df['subgraph'] = subgraphs\n",
    "subgraph_df.to_pickle(save_path)         \n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b97b88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subgraphs(dic,method):\n",
    "    subgraphs={}\n",
    "    for index, (claim, information) in enumerate(dic.items()):\n",
    "        if method=='direct':\n",
    "            subgraph=[]\n",
    "            for entity,relations in information['direct'].items():\n",
    "                for relation,neighbours in information['direct'][entity].items():\n",
    "                    for neighbour in neighbours:\n",
    "                        subgraph.append([entity,relation,neighbour])\n",
    "        if method=='combined':\n",
    "            subgraph=[]\n",
    "            if information['direct'] != None:\n",
    "                for entity,relations in information['direct'].items():\n",
    "                    for relation,neighbours in information['direct'][entity].items():\n",
    "                        for neighbour in neighbours:\n",
    "                            subgraph.append([entity,relation,neighbour])\n",
    "            if 'filtered' in information:\n",
    "                for entity_,relations_ in information['filtered'].items():\n",
    "                    for relation_,neighbours_ in information['filtered'][entity_].items():\n",
    "                        for neighbour_ in neighbours_:\n",
    "                            subgraph.append([entity_,relation_,neighbour_])\n",
    "                        \n",
    "        else:\n",
    "            break\n",
    "        subgraphs[index]=subgraph\n",
    "        #print(f\"{index},{subgraph}\")\n",
    "    return subgraphs\n",
    "#subgraphs=get_subgraphs(testfiltered,'combined')\n",
    "save_path = \"E:\\\\test_combined_subgraphs.pkl\"\n",
    "with open(save_path, \"wb\") as outfile:\n",
    "        pickle.dump(subgraphs, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "03ce10c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import cosine_similarity\n",
    "from torch_geometric.nn import GATConv, global_mean_pool\n",
    "from transformers import AutoModelForSequenceClassification, AutoModel\n",
    "\n",
    "def get_bert_model(model_name=\"bert\", include_classifier=True, num_labels=2, freeze_base_model=False,\n",
    "                   freeze_up_to_pooler=True, dropout_rate=0, use_roberta=False):\n",
    "    \"\"\"\n",
    "    Load a pretrained BERT model with desired configurations.\n",
    "\n",
    "    Args:\n",
    "        model_name (str, optional): Name of the model, will be saved as a class variable. Defaults to \"bert\".\n",
    "        include_classifier (bool, optional): Include a classification layer. Defaults to True.\n",
    "        num_labels (int, optional): Number of outputs if classification layer is included. Defaults to 2.\n",
    "        freeze_base_model (bool, optional): Will freeze all layers up to the classification layer. Defaults to False.\n",
    "        freeze_up_to_pooler (bool, optional): Will freeze all layers until the last layer in BERT, the pooler,\n",
    "            with approximately 500k parameters. Defaults to True.\n",
    "        dropout_rate (int, optional): Dropout rate for the classification layer. Defaults to 0.\n",
    "        use_roberta (bool): If `True`, will use RoBERTa instead of BERT.\n",
    "\n",
    "    Returns:\n",
    "        transformer model: The loaded model.\n",
    "    \"\"\"\n",
    "    if freeze_base_model and freeze_up_to_pooler:\n",
    "        logger.warn(\"Both `freeze_base_model` and `freeze_up_to_pooler` is True. Freezing base model.\")\n",
    "\n",
    "    if use_roberta:\n",
    "        model_name = \"roberta-base\"\n",
    "    else:\n",
    "        model_name = \"bert-base-uncased\"\n",
    "\n",
    "    if include_classifier:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name, cache_dir=\"./cache\", trust_remote_code=True, num_labels=num_labels,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "    else:\n",
    "        model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    model.name = model_name\n",
    "    if freeze_base_model:\n",
    "        for params in model.base_model.parameters():\n",
    "            params.requires_grad = False\n",
    "    elif freeze_up_to_pooler:\n",
    "        for name, params in model.base_model.named_parameters():\n",
    "            if not name.startswith(\"pooler\"):\n",
    "                params.requires_grad = False\n",
    "\n",
    "    if dropout_rate != 0:\n",
    "        classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(model.classifier.in_features, model.classifier.out_features)\n",
    "        )\n",
    "        model.classifier = classifier\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "de8d69e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "import torch\n",
    "#from datasets import get_precomputed_embeddings, get_subgraphs, calculate_embeddings\n",
    "from transformers import AutoTokenizer\n",
    "EMBEDDINGS_FILENAME = \"embeddings.pkl\"\n",
    "DATA_PATH = \"data/\"\n",
    "\n",
    "def calculate_embeddings(text, tokenizer, model, with_classifier=True):\n",
    "    \"\"\"\n",
    "    Calculate embeddings for text, given a tokenizer and a model\n",
    "\n",
    "    Args:\n",
    "        text (list of str): List of the strings to make embeddings for.\n",
    "        tokenizer (tokenizer): The tokenizer.\n",
    "        model (pytorch model): The model.\n",
    "        with_classifier (bool): If model has classifier (should reach hidden state) or not (output is hidden state).\n",
    "\n",
    "    Returns:\n",
    "        dict: Dict mapping from text to embeddings.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\")\n",
    "    inputs = {key: val.to(model.device) for key, val in inputs.items()}  # Move to device\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Get the embedding for the [CLS] token (first token)\n",
    "    if with_classifier:\n",
    "        last_hidden_states = outputs.hidden_states[-1]\n",
    "    else:\n",
    "        last_hidden_states = outputs.last_hidden_state\n",
    "    cls_embedding = last_hidden_states[:, 0, :]\n",
    "    return cls_embedding\n",
    "\n",
    "def get_entity_and_relations_list(graph):\n",
    "    nodes = []\n",
    "    edges = []\n",
    "    for entity, relations in graph:\n",
    "        for relation in relations:\n",
    "            if relation not in edges:\n",
    "                edges.append(relation)\n",
    "        if entity not in nodes:\n",
    "            nodes.append(entity)\n",
    "    return nodes,edges\n",
    "            \n",
    "\n",
    "def get_all_embeddings(subgraph_df, tokenizer, model, batch_size=32):\n",
    "    file_path = Path(DATA_PATH) / EMBEDDINGS_FILENAME\n",
    "    embedding_dict = {}\n",
    "\n",
    "    all_entities_and_relations = set()  # Get all text (entities and relations) from all graphs\n",
    "    for graph in subgraph_df['subgraph']:\n",
    "        entities, relations = entity(graph)\n",
    "        all_entities_and_relations.update(entities)\n",
    "        all_entities_and_relations.update(relations)\n",
    "\n",
    "    # Convert set to list and remove already computed embeddings\n",
    "    all_text = [item for item in all_entities_and_relations if item not in embedding_dict]\n",
    "\n",
    "    # Get embeddings in batches.\n",
    "    n_text_from_df = len(all_entities_and_relations)\n",
    "    n_text = len(all_text)\n",
    "    for i in range(0, n_text, batch_size):\n",
    "        if (i % 1) == 0:\n",
    "            print(f\"On idx {i}/{n_text}\")\n",
    "        batch_texts = all_text[i:i + batch_size]\n",
    "        embeddings = calculate_embeddings(batch_texts, tokenizer, model)\n",
    "        for text, embedding in zip(batch_texts, embeddings):\n",
    "            embedding_dict[text] = embedding.cpu().numpy()  # Move embeddings to CPU and convert to numpy for storage\n",
    "\n",
    "    with open(file_path, \"wb\") as outfile:\n",
    "        pickle.dump(embedding_dict, outfile)\n",
    "\n",
    "    return embedding_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b194a820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: False\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[115], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA Available:\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available())\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA Device Name:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_device_name\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\pyg_env\\lib\\site-packages\\torch\\cuda\\__init__.py:493\u001b[0m, in \u001b[0;36mget_device_name\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    481\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_device_name\u001b[39m(device: Optional[_device_t] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    482\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Get the name of a device.\u001b[39;00m\n\u001b[0;32m    483\u001b[0m \n\u001b[0;32m    484\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    491\u001b[0m \u001b[38;5;124;03m        str: the name of the device\u001b[39;00m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 493\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_device_properties\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mname\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\pyg_env\\lib\\site-packages\\torch\\cuda\\__init__.py:523\u001b[0m, in \u001b[0;36mget_device_properties\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_device_properties\u001b[39m(device: _device_t) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _CudaDeviceProperties:\n\u001b[0;32m    514\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Get the properties of a device.\u001b[39;00m\n\u001b[0;32m    515\u001b[0m \n\u001b[0;32m    516\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;124;03m        _CudaDeviceProperties: the properties of the device\u001b[39;00m\n\u001b[0;32m    522\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 523\u001b[0m     \u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# will define _get_device_properties\u001b[39;00m\n\u001b[0;32m    524\u001b[0m     device \u001b[38;5;241m=\u001b[39m _get_device_index(device, optional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    525\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count():\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\pyg_env\\lib\\site-packages\\torch\\cuda\\__init__.py:310\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m     )\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    313\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    314\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"CUDA Device Name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "fedc90f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e93828f0eb14a949c0ecbf7f66f255b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  45%|####5     | 199M/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[114], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(device)\n\u001b[0;32m      3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mget_bert_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbert\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      5\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m get_all_embeddings(subgraph_df, tokenizer, model, batch_size\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mbatch_size)\n",
      "Cell \u001b[1;32mIn[106], line 35\u001b[0m, in \u001b[0;36mget_bert_model\u001b[1;34m(model_name, include_classifier, num_labels, freeze_base_model, freeze_up_to_pooler, dropout_rate, use_roberta)\u001b[0m\n\u001b[0;32m     32\u001b[0m     model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_classifier:\n\u001b[1;32m---> 35\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./cache\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     40\u001b[0m     model \u001b[38;5;241m=\u001b[39m AutoModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\pyg_env\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    565\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    566\u001b[0m     )\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    570\u001b[0m )\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\pyg_env\\lib\\site-packages\\transformers\\modeling_utils.py:3776\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3760\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   3761\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m   3762\u001b[0m     cached_file_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   3763\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m: cache_dir,\n\u001b[0;32m   3764\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforce_download\u001b[39m\u001b[38;5;124m\"\u001b[39m: force_download,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3774\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m: commit_hash,\n\u001b[0;32m   3775\u001b[0m     }\n\u001b[1;32m-> 3776\u001b[0m     resolved_archive_file \u001b[38;5;241m=\u001b[39m cached_file(pretrained_model_name_or_path, filename, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcached_file_kwargs)\n\u001b[0;32m   3778\u001b[0m     \u001b[38;5;66;03m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[39;00m\n\u001b[0;32m   3779\u001b[0m     \u001b[38;5;66;03m# result when internet is up, the repo and revision exist, but the file does not.\u001b[39;00m\n\u001b[0;32m   3780\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_archive_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m filename \u001b[38;5;241m==\u001b[39m _add_variant(SAFE_WEIGHTS_NAME, variant):\n\u001b[0;32m   3781\u001b[0m         \u001b[38;5;66;03m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\pyg_env\\lib\\site-packages\\transformers\\utils\\hub.py:403\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    400\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 403\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    418\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\pyg_env\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\pyg_env\\lib\\site-packages\\huggingface_hub\\file_download.py:860\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[0;32m    840\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[0;32m    841\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[0;32m    842\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    857\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    858\u001b[0m     )\n\u001b[0;32m    859\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 860\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[0;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[0;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[0;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[0;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\pyg_env\\lib\\site-packages\\huggingface_hub\\file_download.py:1009\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[1;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[0;32m   1007\u001b[0m Path(lock_path)\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1008\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[1;32m-> 1009\u001b[0m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.incomplete\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1014\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1019\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(pointer_path):\n\u001b[0;32m   1020\u001b[0m         _create_symlink(blob_path, pointer_path, new_blob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\pyg_env\\lib\\site-packages\\huggingface_hub\\file_download.py:1543\u001b[0m, in \u001b[0;36m_download_to_tmp_and_move\u001b[1;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download)\u001b[0m\n\u001b[0;32m   1540\u001b[0m         _check_disk_space(expected_size, incomplete_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[0;32m   1541\u001b[0m         _check_disk_space(expected_size, destination_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[1;32m-> 1543\u001b[0m     \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1544\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1545\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1550\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1552\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownload complete. Moving file to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1553\u001b[0m _chmod_and_move(incomplete_path, destination_path)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\pyg_env\\lib\\site-packages\\huggingface_hub\\file_download.py:452\u001b[0m, in \u001b[0;36mhttp_get\u001b[1;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[0;32m    450\u001b[0m new_resume_size \u001b[38;5;241m=\u001b[39m resume_size\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 452\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39mconstants\u001b[38;5;241m.\u001b[39mDOWNLOAD_CHUNK_SIZE):\n\u001b[0;32m    453\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk:  \u001b[38;5;66;03m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[0;32m    454\u001b[0m             progress\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\pyg_env\\lib\\site-packages\\requests\\models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\pyg_env\\lib\\site-packages\\urllib3\\response.py:1060\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m   1058\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1059\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1060\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1062\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[0;32m   1063\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\pyg_env\\lib\\site-packages\\urllib3\\response.py:949\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    946\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[0;32m    947\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[1;32m--> 949\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    951\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[0;32m    953\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\pyg_env\\lib\\site-packages\\urllib3\\response.py:873\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[1;34m(self, amt, read1)\u001b[0m\n\u001b[0;32m    870\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    872\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[1;32m--> 873\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    874\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[0;32m    875\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[0;32m    876\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[0;32m    883\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\pyg_env\\lib\\site-packages\\urllib3\\response.py:856\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[1;34m(self, amt, read1)\u001b[0m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1()\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\pyg_env\\lib\\http\\client.py:463\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    461\u001b[0m     \u001b[38;5;66;03m# Amount is given, implement using readinto\u001b[39;00m\n\u001b[0;32m    462\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytearray\u001b[39m(amt)\n\u001b[1;32m--> 463\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(b)[:n]\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    466\u001b[0m     \u001b[38;5;66;03m# Amount is not given (unbounded read) so we must check self.length\u001b[39;00m\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;66;03m# and self.chunked\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\pyg_env\\lib\\http\\client.py:507\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    502\u001b[0m         b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmemoryview\u001b[39m(b)[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength]\n\u001b[0;32m    504\u001b[0m \u001b[38;5;66;03m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;66;03m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[0;32m    506\u001b[0m \u001b[38;5;66;03m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[1;32m--> 507\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m n \u001b[38;5;129;01mand\u001b[39;00m b:\n\u001b[0;32m    509\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    510\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    511\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\pyg_env\\lib\\socket.py:716\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    714\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    715\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 716\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    717\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    718\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\pyg_env\\lib\\ssl.py:1275\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1271\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1272\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1273\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1274\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1276\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\pyg_env\\lib\\ssl.py:1133\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1133\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1134\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1135\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "subgraph_df=p\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = get_bert_model(\"bert\").to(device)\n",
    "embeddings = get_all_embeddings(subgraph_df, tokenizer, model, batch_size=args.batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223fa536",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b07547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch_geometric.data import Batch, Data\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from constants import (BERT_LAST_LAYER_DIM, DATA_PATH, DATA_SPLIT_FILENAMES, EMBEDDINGS_FILENAME, FACTKG_FOLDER,\n",
    "                       SUBGRAPH_FOLDER)\n",
    "from glocal_settings import SMALL, SMALL_SIZE\n",
    "from utils import get_logger\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_precomputed_embeddings():#just get the embedding before\n",
    "    \"\"\"\n",
    "    Gets dict with precomputed embeddings, made with `make_subgraph_embeddings.py`.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If `data_split` is an unsuported string.\n",
    "\n",
    "    Returns:\n",
    "        dict: The dict of the subgraphs (as strings).\n",
    "    \"\"\"\n",
    "    path = Path(DATA_PATH) / EMBEDDINGS_FILENAME\n",
    "    embedding_dict = pickle.load(open(path, \"rb\"))\n",
    "    return embedding_dict\n",
    "\n",
    "\n",
    "class FactKGDataset(Dataset):\n",
    "    def __init__(self, df, evidence=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df (pd.DataFrame): Dataframe with claims (\"Sentence\") and labels (\"Label\").\n",
    "            evidence (list, optional): List of the subgraph evidences to use, will be converted to string. `None`\n",
    "                if no evidence should be used.\n",
    "        \"\"\"\n",
    "\n",
    "        self.inputs = df[\"Sentence\"]\n",
    "        self.labels = [int(label[0]) for label in df[\"Label\"]]\n",
    "        self.length = len(df)\n",
    "\n",
    "        if evidence is not None:\n",
    "            self.inputs = [self.inputs[i] + \" | \" + str(evidence[i]) for i in range(self.length)]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.labels[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "\n",
    "class CollateFunctor:\n",
    "    def __init__(self, tokenizer, max_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        inputs, labels = zip(*batch)\n",
    "        labels = torch.tensor(labels)\n",
    "        inputs = self.tokenizer(inputs, return_tensors=\"pt\", padding=True, truncation=True, max_length=self.max_length)\n",
    "        inputs[\"labels\"] = torch.as_tensor(labels)\n",
    "        return inputs\n",
    "\n",
    "\n",
    "def get_dataloader(data_split, subgraph_type=None, subgraph_to_use=\"discovered\", model=\"bert-base-uncased\",\n",
    "                   max_length=512, batch_size=64, shuffle=True, drop_last=True):\n",
    "    \"\"\"\n",
    "    Creates a dataloader for the desired data split and evidence (subgraph).\n",
    "\n",
    "    Args:\n",
    "        data_split (str): Which datasplit to load, in `train`, `val` or `test`\n",
    "        subgraph_type (str): The type of subgraph to use. Must be either `direct` (only the direct entity\n",
    "            neighbours), `direct_filled` (the direct entity neigbhours, but if it is empty, replace it with\n",
    "            all of the entity edges if the entities) or `one_hop` (all of the entity edges).\n",
    "        subgraph_to_use (str). In [\"discovered\", \"connected\", \"walkable\"]. \"discovered\" means that we use the string\n",
    "            representation of what directly found with `subgraph_type`. \"Connected\" means that walk the nodes and\n",
    "            relations found with `subgraph_type`, and use the connected graphs if found, and the walkable if not.\n",
    "            \"walkable\" means we use both the connected graphs and the walkable graphs.\n",
    "        model (str, optional): Name of model, in order to get tokenizer. Defaults to \"bert-base-uncased\".\n",
    "        max_length (int, optional): Max tokenizer length. Defaults to 512.\n",
    "        batch_size (int, optional): Batch size to dataloader. Defaults to 128.\n",
    "        shuffle (bool, optional): Shuffle dataset. Defaults to True.\n",
    "        drop_last (bool, optional): Drop last batch if it is less than `batch_size`. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        DataLoader: The dataloader.\n",
    "    \"\"\"\n",
    "    df = get_df(data_split)\n",
    "    if subgraph_type is not None:\n",
    "        subgraphs = get_subgraphs(data_split, subgraph_type)\n",
    "        choices = [\"discovered\", \"connected\", \"walkable\"]\n",
    "        if subgraph_to_use not in choices:\n",
    "            raise ValueError(f\"Argument `subgraph_to_use` must be in {choices}. Was {subgraph_to_use}. \")\n",
    "        if subgraph_to_use == \"discovered\":\n",
    "            evidence = subgraphs[\"subgraph\"]\n",
    "    else:\n",
    "        evidence = None\n",
    "\n",
    "    dataset = FactKGDataset(df, evidence)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    collate_func = CollateFunctor(tokenizer, max_length)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last,\n",
    "                            collate_fn=collate_func)\n",
    "    return dataloader\n",
    "\n",
    "BERT_LAST_LAYER_DIM=[768]\n",
    "def get_embedding(text, embeddings_dict, tokenizer, model):\n",
    "\n",
    "    if embeddings_dict.get(text) is not None:\n",
    "        return torch.tensor(embeddings_dict[text])\n",
    "    return torch.zeros(BERT_LAST_LAYER_DIM)\n",
    "\n",
    "\n",
    "def convert_to_pyg_format(graph, online_embeddings, embedding_dict=None, tokenizer=None, model=None):\n",
    "    \"\"\"\n",
    "    Convert graph on DBpedia dict format to torch_embedding.data format, so it can be run in GNN.\n",
    "\n",
    "    Args:\n",
    "        graph (dict): Dict of graph, gotten by calling `kg.search()` on each element in the graph.\n",
    "        online_embeddings (bool): If True, will calculate embeddings for knowledge subgraph online, with a model\n",
    "                that might be tuned during the training.\n",
    "        embedding_dict (dict): Dict mapping words to embeddings, to be used as node and edge features.\n",
    "            This should be precomputed.\n",
    "        tokenizer (tokenizer): Tokenizer to `model` if `online_embeddings`.\n",
    "        model (pytroch model): Model to compute embeddings if `online_embeddings`.\n",
    "\n",
    "    Returns:\n",
    "        torch_geometric.data: Graph data.\n",
    "    \"\"\"\n",
    "\n",
    "    if graph == []:  # Dummy empty graph. Not actually empty because of vectorized computations.\n",
    "        graph = [[\"none\", \"none\", \"none\"]]\n",
    "    node_to_index = {}  # Node text to int mapping\n",
    "    edge_to_index = {}  # Same for edges\n",
    "    node_features = []  # List of embeddings\n",
    "    edge_features = []  # Same for edges\n",
    "    edge_indices = []\n",
    "\n",
    "    current_node_idx = 0\n",
    "    current_edge_idx = 0\n",
    "    for edge_list in graph:\n",
    "        node1, edge, node2 = edge_list  # Graph consists of list on the format [node1, edge, node2]\n",
    "\n",
    "        if node1 not in node_to_index:\n",
    "            node_to_index[node1] = current_node_idx\n",
    "            embedding = get_embedding(node1, embeddings_dict=embedding_dict,\n",
    "                                      tokenizer=tokenizer, model=model)\n",
    "            node_features.append(embedding)\n",
    "            current_node_idx += 1\n",
    "\n",
    "        if node2 not in node_to_index:\n",
    "            node_to_index[node2] = current_node_idx\n",
    "            embedding = get_embedding(node2, embeddings_dict=embedding_dict,\n",
    "                                      tokenizer=tokenizer, model=model)\n",
    "            node_features.append(embedding)\n",
    "            current_node_idx += 1\n",
    "\n",
    "        if edge not in edge_to_index:\n",
    "            edge_to_index[edge] = current_edge_idx\n",
    "            embedding = get_embedding(edge, embeddings_dict=embedding_dict,\n",
    "                                      tokenizer=tokenizer, model=model)\n",
    "            edge_features.append(embedding)\n",
    "            current_edge_idx += 1\n",
    "\n",
    "        edge_indices.append([node_to_index[node1], node_to_index[node2]])\n",
    "\n",
    "    edge_index = torch.tensor(edge_indices).t().contiguous()  # Transpose and make memory contigious\n",
    "    x = torch.stack(node_features)\n",
    "    edge_attr = torch.stack(edge_features)\n",
    "\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "    return data\n",
    "\n",
    "\n",
    "class FactKGDatasetGraph(Dataset):\n",
    "    def __init__(self, df, evidence, embedding_dict=None, tokenizer=None, model=None):\n",
    "        \"\"\"\n",
    "        Initialize the dataset. This dataset will return tokenized claims, graphs for the subgraph, and labels.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): FactKG dataframe\n",
    "            evidence (pd.DataFram): Dataframe with the subgraphs, found by `retrieve_subgraphs.py`.\n",
    "            online_embeddings (bool): If True, will calculate embeddings for knowledge subgraph online, with a model\n",
    "                that might be tuned during the training.\n",
    "            embedding_dict (dict): Dict mapping the knowledge graph words to embeddings if not `online_embeddings`.\n",
    "            tokenizer (tokenizer): Tokenizer to `model` if `online_embeddings`.\n",
    "            model (pytroch model): Model to compute embeddings if `online_embeddings`.\n",
    "            mix_graphs (bool, optional): If `True`, will use both the connected and the walkable graphs found in\n",
    "                DBpedia. If `False`, will use connected if it is not empty, else walkable. Defaults to False.\n",
    "        \"\"\"\n",
    "        self.inputs = df[\"Sentence\"]\n",
    "        self.labels = [int(label[0]) for label in df[\"Label\"]]\n",
    "        self.length = len(df)\n",
    "        self.subgraphs = evidence[\"walked\"]\n",
    "        self.evidence = evidence[\"subgraph\"]\n",
    "        self.online_embeddings = online_embeddings\n",
    "        self.embedding_dict = embedding_dict\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.mix_graphs = mix_graphs\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        claims = self.inputs[idx]\n",
    "        walked = self.subgraphs[idx]\n",
    "        graph = convert_to_pyg_format(\n",
    "            subgraph, online_embeddings=self.online_embeddings, embedding_dict=self.embedding_dict,\n",
    "            tokenizer=self.tokenizer, model=self.model)\n",
    "\n",
    "        label = self.labels[idx]\n",
    "        return claims, graph, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "\n",
    "class GraphCollateFunc:\n",
    "    def __init__(self, tokenizer, max_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        inputs, graph_batch, labels = zip(*batch)\n",
    "        tokens = self.tokenizer(inputs, return_tensors=\"pt\", padding=True, truncation=True,\n",
    "                                max_length=self.max_length)\n",
    "        graph_batch = Batch.from_data_list(graph_batch)\n",
    "        labels = torch.tensor(labels).float()\n",
    "\n",
    "        return tokens, graph_batch, labels\n",
    "\n",
    "\n",
    "def get_graph_dataloader(\n",
    "        data_split, subgraph_type, online_embeddings=False, model=None, bert_model_name=\"bert-base-uncased\",\n",
    "        max_length=512, batch_size=64, shuffle=True, drop_last=True, mix_graphs=False):\n",
    "    \"\"\"\n",
    "    Creates a dataloader for dataset with subgraph representation and tokenized text.\n",
    "\n",
    "    Args:\n",
    "        data_split (str): Which datasplit to load, in `train`, `val` or `test`\n",
    "        subgraph_type (str): The type of subgraph to use. Must be either `direct` (only the direct entity\n",
    "            neighbours), `direct_filled` (the direct entity neigbhours, but if it is empty, replace it with\n",
    "            all of the entity edges if the entities), `one_hop` (all of the entity edges) or `relevant` (direct plus\n",
    "            edges that appears in claim).\n",
    "        online_embeddings (bool): If True, will calculate embeddings for knowledge subgraph online, with a model\n",
    "                that might be tuned during the training.\n",
    "        bert_model_name (str, optional): Name of model, in order to get tokenizer. Defaults to \"bert-base-uncased\".\n",
    "        max_length (int, optional): Max tokenizer length. Defaults to 512.\n",
    "        batch_size (int, optional): Batch size to dataloader. Defaults to 128.\n",
    "        shuffle (bool, optional): Shuffle dataset. Defaults to True.\n",
    "        drop_last (bool, optional): Drop last batch if it is less than `batch_size`. Defaults to True.\n",
    "        mix_graphs (bool, optional): If `True`, will use both the connected and the walkable graphs found in\n",
    "                DBpedia. If `False`, will use connected if it is not empty, else walkable. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        DataLoader: The dataloader.\n",
    "    \"\"\"\n",
    "    df = get_df(data_split)\n",
    "    subgraphs = get_subgraphs(data_split, subgraph_type)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "    graph_collate_func = GraphCollateFunc(tokenizer, max_length=max_length)\n",
    "    embedding_dict = embeddings\n",
    "    dataset = FactKGDatasetGraph(\n",
    "            df, subgraphs, embedding_dict=embedding_dict, mix_graphs=mix_graphs)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last,\n",
    "                            collate_fn=graph_collate_func)\n",
    "    return dataloader\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pyg_env)",
   "language": "python",
   "name": "pyg_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
